"""Command-line interface for managing dynamic prompt embeddings."""

import argparse
import json
import sys
from datetime import datetime
from pathlib import Path
from typing import Optional, Dict, Any

from ..core.cache import precompute_all_embeddings
from ..core.manager import DynamicPromptManager


def calculate_prompt_metrics(base_prompt: str, dynamic_prompt: str) -> Dict[str, Any]:
    """Calculate comparison metrics between base and dynamic prompts."""
    base_lines = base_prompt.count('\n') + 1
    dynamic_lines = dynamic_prompt.count('\n') + 1
    
    base_words = len(base_prompt.split())
    dynamic_words = len(dynamic_prompt.split())
    
    base_chars = len(base_prompt)
    dynamic_chars = len(dynamic_prompt)
    
    return {
        "base_prompt": {
            "lines": base_lines,
            "words": base_words,
            "characters": base_chars,
        },
        "dynamic_prompt": {
            "lines": dynamic_lines,
            "words": dynamic_words,
            "characters": dynamic_chars,
        },
        "comparison": {
            "lines_added": dynamic_lines - base_lines,
            "words_added": dynamic_words - base_words,
            "characters_added": dynamic_chars - base_chars,
            "size_increase_percent": round(((dynamic_chars - base_chars) / base_chars) * 100, 1) if base_chars > 0 else 0,
        }
    }


def generate_markdown_report(query: str, entity_mapping: Dict[str, str], metrics: Dict[str, Any], 
                           similar_examples: list, base_prompt: str, dynamic_prompt: str) -> str:
    """Generate a markdown report for prompt comparison."""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    report = f"""# Dynamic Prompt Analysis Report

**Generated:** {timestamp}  
**Query:** {query}

## Entity Normalization

"""
    
    if entity_mapping:
        report += "**Entities Detected:**\n"
        for placeholder, entity in entity_mapping.items():
            report += f"- `{placeholder}` ‚Üí {entity}\n"
    else:
        report += "**No entities detected**\n"
    
    report += f"""

## Prompt Metrics Comparison

| Metric | Base Prompt | Dynamic Prompt | Change |
|--------|-------------|----------------|---------|
| Lines | {metrics['base_prompt']['lines']} | {metrics['dynamic_prompt']['lines']} | +{metrics['comparison']['lines_added']} |
| Words | {metrics['base_prompt']['words']} | {metrics['dynamic_prompt']['words']} | +{metrics['comparison']['words_added']} |
| Characters | {metrics['base_prompt']['characters']} | {metrics['dynamic_prompt']['characters']} | +{metrics['comparison']['characters_added']} |
| Size Increase | - | - | {metrics['comparison']['size_increase_percent']}% |

## Similar Examples Found

"""
    
    if similar_examples:
        for i, example in enumerate(similar_examples, 1):
            similarity = example.get('similarity_score', 0)
            query_text = example.get('query', 'Unknown')
            tool_name = example.get('tool_name', 'Unknown')
            report += f"{i}. **{query_text}** (similarity: {similarity:.3f})\n"
            report += f"   - Tool: `{tool_name}`\n\n"
    else:
        report += "No similar examples found.\n"
    
    report += f"""
## Base Prompt

```
{base_prompt}
```

## Dynamic Prompt (with examples)

```
{dynamic_prompt}
```

---
*Generated by kfinance Dynamic Prompt Construction System*
"""
    
    return report


def precompute_command(args: argparse.Namespace) -> None:
    """Precompute embeddings for all examples."""
    print("üîÑ Precomputing embeddings...")
    
    try:
        precompute_all_embeddings(
            examples_dir=args.examples_dir,
            cache_dir=args.cache_dir,
            embedding_model=args.model,
            force_recompute=args.force,
        )
        print("‚úÖ Successfully precomputed embeddings!")
        
    except Exception as e:
        print(f"‚ùå Failed to precompute embeddings: {e}")
        sys.exit(1)


def stats_command(args: argparse.Namespace) -> None:
    """Show cache and repository statistics."""
    print("üìä Dynamic Prompt Statistics")
    print("=" * 40)
    
    try:
        manager = DynamicPromptManager(
            examples_dir=args.examples_dir,
            cache_dir=args.cache_dir,
            embedding_model=args.model,
        )
        
        stats = manager.get_repository_stats()
        
        # Repository stats
        print(f"Total examples: {stats.get('total_examples', 0)}")
        print(f"Parameter descriptors: {stats.get('total_parameter_descriptors', 0)}")
        print(f"Tools with descriptors: {stats.get('tools_with_descriptors', 0)}")
        
        # Examples by tool
        examples_by_tool = stats.get('examples_by_tool', {})
        if examples_by_tool:
            print("\nExamples by tool:")
            for tool, count in examples_by_tool.items():
                print(f"  {tool}: {count}")
        
        # Cache stats
        cache_stats = stats.get('cache', {})
        if cache_stats and 'error' not in cache_stats:
            print(f"\nCache Statistics:")
            print(f"  Cached embeddings: {cache_stats.get('cached_embeddings', 0)}")
            print(f"  Cache size: {cache_stats.get('cache_size_mb', 0)} MB")
            print(f"  Model: {cache_stats.get('model_name', 'unknown')}")
            print(f"  Last updated: {cache_stats.get('last_updated', 'never')}")
            print(f"  Cache directory: {cache_stats.get('cache_dir', 'unknown')}")
        else:
            print("\n‚ö†Ô∏è  No cache statistics available")
        
    except Exception as e:
        print(f"‚ùå Failed to get statistics: {e}")
        sys.exit(1)


def invalidate_command(args: argparse.Namespace) -> None:
    """Invalidate the embedding cache."""
    print("üóëÔ∏è  Invalidating embedding cache...")
    
    try:
        manager = DynamicPromptManager(
            examples_dir=args.examples_dir,
            cache_dir=args.cache_dir,
            embedding_model=args.model,
        )
        
        if manager.invalidate_cache():
            print("‚úÖ Successfully invalidated cache!")
        else:
            print("‚ùå Failed to invalidate cache")
            sys.exit(1)
        
    except Exception as e:
        print(f"‚ùå Failed to invalidate cache: {e}")
        sys.exit(1)


def test_command(args: argparse.Namespace) -> None:
    """Test dynamic prompt construction with a sample query."""
    print("üß™ Testing dynamic prompt construction...")
    
    try:
        from kfinance.client.permission_models import Permission
        from kfinance.integrations.tool_calling.prompts import BASE_PROMPT
        
        manager = DynamicPromptManager(
            examples_dir=args.examples_dir,
            cache_dir=args.cache_dir,
            embedding_model=args.model,
        )
        
        # Test query
        test_query = args.query or "What is the preferred stock additional paid in capital for Apple?"
        user_permissions = {Permission.StatementsPermission}
        
        print(f"Query: {test_query}")
        
        # Show entity normalization
        entity_mapping = {}
        try:
            # Access the repository to get the entity processor
            manager._initialize()  # Ensure repository is loaded
            if hasattr(manager._repository, 'normalize_query_for_search'):
                normalized_query, entity_mapping = manager._repository.normalize_query_for_search(test_query)
                if entity_mapping:
                    print(f"Normalized: {normalized_query}")
                    print(f"Entity mapping: {entity_mapping}")
                else:
                    print("Normalized: (no entities detected)")
        except Exception as e:
            print(f"Entity normalization: (error: {e})")
        
        print("-" * 60)
        
        # Get both base and dynamic prompts
        base_prompt = BASE_PROMPT
        dynamic_prompt, stats = manager.get_prompt_with_stats(
            query=test_query,
            user_permissions=user_permissions,
        )
        
        # Calculate prompt comparison metrics
        metrics = calculate_prompt_metrics(base_prompt, dynamic_prompt)
        
        # Show prompt comparison metrics
        print("üìä Prompt Comparison Metrics:")
        print(f"  Base prompt:    {metrics['base_prompt']['lines']} lines, {metrics['base_prompt']['words']} words, {metrics['base_prompt']['characters']} chars")
        print(f"  Dynamic prompt: {metrics['dynamic_prompt']['lines']} lines, {metrics['dynamic_prompt']['words']} words, {metrics['dynamic_prompt']['characters']} chars")
        print(f"  Size increase:  +{metrics['comparison']['lines_added']} lines, +{metrics['comparison']['words_added']} words, +{metrics['comparison']['characters_added']} chars ({metrics['comparison']['size_increase_percent']}%)")
        
        print("\nüìà Dynamic Prompt Statistics:")
        for key, value in stats.items():
            print(f"  {key}: {value}")
        
        # Show similar examples
        similar_examples = manager.search_similar_examples(
            query=test_query,
            user_permissions=user_permissions,
            top_k=3,
        )
        
        if similar_examples:
            print(f"\nüîç Top {len(similar_examples)} similar examples:")
            for i, example in enumerate(similar_examples, 1):
                similarity = example.get('similarity_score', 0)
                query_text = example.get('query', 'Unknown')
                tool_name = example.get('tool_name', 'Unknown')
                print(f"  {i}. {query_text} (similarity: {similarity:.3f}) - {tool_name}")
        else:
            print("\n‚ùå No similar examples found")
        
        # Generate markdown report if requested
        if args.output_markdown:
            report = generate_markdown_report(
                query=test_query,
                entity_mapping=entity_mapping,
                metrics=metrics,
                similar_examples=similar_examples,
                base_prompt=base_prompt,
                dynamic_prompt=dynamic_prompt
            )
            
            output_path = Path(args.output_markdown)
            with open(output_path, 'w') as f:
                f.write(report)
            print(f"\nüìÑ Markdown report saved to: {output_path}")
        
        if args.show_prompt:
            print(f"\nüìù Generated Dynamic Prompt (first 500 chars):")
            print("-" * 60)
            print(dynamic_prompt[:500] + "..." if len(dynamic_prompt) > 500 else dynamic_prompt)
        
    except Exception as e:
        print(f"‚ùå Test failed: {e}")
        sys.exit(1)


def main():
    """Main CLI entry point."""
    parser = argparse.ArgumentParser(
        description="Dynamic Prompt Construction CLI",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Precompute embeddings for all examples
  python -m kfinance.integrations.tool_calling.dynamic_prompts.cli precompute
  
  # Show statistics
  python -m kfinance.integrations.tool_calling.dynamic_prompts.cli stats
  
  # Test with a custom query and show metrics
  python -m kfinance.integrations.tool_calling.dynamic_prompts.cli test --query "What is the revenue for Apple?"
  
  # Test with markdown report output
  python -m kfinance.integrations.tool_calling.dynamic_prompts.cli test --query "What is Apple's debt ratio?" --output-markdown report.md
  
  # Force recompute all embeddings
  python -m kfinance.integrations.tool_calling.dynamic_prompts.cli precompute --force
        """
    )
    
    # Global arguments
    parser.add_argument(
        "--examples-dir", 
        type=Path, 
        help="Directory containing example JSON files"
    )
    parser.add_argument(
        "--cache-dir", 
        type=Path, 
        help="Directory to store cache files"
    )
    parser.add_argument(
        "--model", 
        default="sentence-transformers/all-MiniLM-L6-v2",
        help="Embedding model name"
    )
    
    # Subcommands
    subparsers = parser.add_subparsers(dest="command", help="Available commands")
    
    # Precompute command
    precompute_parser = subparsers.add_parser(
        "precompute", 
        help="Precompute embeddings for all examples"
    )
    precompute_parser.add_argument(
        "--force", 
        action="store_true", 
        help="Force recomputation of all embeddings"
    )
    
    # Stats command
    subparsers.add_parser("stats", help="Show cache and repository statistics")
    
    # Invalidate command
    subparsers.add_parser("invalidate", help="Invalidate the embedding cache")
    
    # Test command
    test_parser = subparsers.add_parser("test", help="Test dynamic prompt construction with entity normalization and metrics")
    test_parser.add_argument(
        "--query", 
        help="Query to test with (default: sample query). Shows entity normalization using spaCy NER."
    )
    test_parser.add_argument(
        "--show-prompt", 
        action="store_true", 
        help="Show the generated prompt"
    )
    test_parser.add_argument(
        "--output-markdown",
        help="Output detailed comparison report to markdown file (e.g., report.md)"
    )
    
    # Parse arguments
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        sys.exit(1)
    
    # Execute command
    if args.command == "precompute":
        precompute_command(args)
    elif args.command == "stats":
        stats_command(args)
    elif args.command == "invalidate":
        invalidate_command(args)
    elif args.command == "test":
        test_command(args)
    else:
        print(f"Unknown command: {args.command}")
        sys.exit(1)


if __name__ == "__main__":
    main()
